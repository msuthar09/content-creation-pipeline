{"cells":[{"cell_type":"markdown","metadata":{},"source":[" ## Global Imports And Library Setup"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# # Uncomment below to install missing packages\n","# !python -m pip install --upgrade --requirement ./requirements.txt\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### Login to Azure CLI"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# # Login to Azure CLI\n","# !az login --use-device-code --tenant \"38b2262e-92fe-4b71-a4d0-ebf91a3e2909\"\n","# !az account set --subscription \"47c2af6c-fe2f-4dbd-9193-9b50a99044b7\"\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### Import Python libraries"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import re\n","import ast\n","import json\n","import time\n","import uuid\n","import openai\n","import logging\n","import requests\n","import pandas as pd\n","from PIL import Image\n","from io import BytesIO\n","from pathlib import Path\n","from tqdm.auto import tqdm\n","from dotenv import load_dotenv\n","from azure.identity import DefaultAzureCredential\n","from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n","\n","# Load environment variables from `.env` file\n","load_dotenv(verbose=True, override=True)\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### Setup environment constants"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Azure container name\n","CONTAINER_NAME = os.environ.get(\"CONTAINER_NAME\", \"content-creation-pipeline\")\n","\n","try:\n","    SYNC_BLOB_TO_LOCAL_FORCE_DOWNLOAD = ast.literal_eval(\n","        os.environ.get(\"SYNC_BLOB_TO_LOCAL_FORCE_DOWNLOAD\", \"False\")\n","    )\n","except:\n","    SYNC_BLOB_TO_LOCAL_FORCE_DOWNLOAD = False\n"]},{"cell_type":"markdown","metadata":{},"source":[" ## Clean Azure Data (If Needed)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# # Used to clean up the blob if doing a bulk-delete\n","# ################################################################################################\n","# container_client = blob_service_client.get_container_client(container=CONTAINER_NAME)\n","# for n in container_client.list_blob_names():\n","#     if not n.startswith(\"data\"):\n","#         continue\n","#     folderSplits = n.split(\"/\")\n","#     if len(folderSplits) != 3:\n","#         continue\n","#     try:\n","#         puzzleId = int(n.split(\"/\")[1], base=10)\n","#         if puzzleId > 12:\n","#             print(f\"Deleting blob = {n}\")\n","#             blob_service_client.get_blob_client(container=container_name, blob=n).delete_blob()\n","#     except ValueError as ex:\n","#         if ex.args[0].startswith(\"invalid literal for int() with base 10\"):\n","#             # This is an ad-hoc file. And not a puzzle folder\n","#             continue\n","#         else:\n","#             print(ex)\n","#             break\n"]},{"cell_type":"markdown","metadata":{},"source":[" ## Setup Azure Libraries"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# account_url = \"https://rakhdelstudioapps.blob.core.windows.net\"\n","# default_credential = DefaultAzureCredential(\n","#     exclude_environment_credential=True,\n","#     exclude_managed_identity_credential=True,\n","#     exclude_visual_studio_code_credential=True,\n","#     exclude_shared_token_cache_credential=True,\n","#     interactive_browser_tenant_id=\"38b2262e-92fe-4b71-a4d0-ebf91a3e2909\",\n","# )\n","\n","# # Create the BlobServiceClient object\n","# blob_service_client = BlobServiceClient(account_url, credential=default_credential)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["import azure.cognitiveservices.speech as speechsdk\n","import wave\n","\n","# This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n","speech_config = speechsdk.SpeechConfig(\n","    subscription=os.environ.get(\"SPEECH_KEY\"),\n","    region=os.environ.get(\"SPEECH_REGION\"),\n","    speech_recognition_language=\"gu-IN-DhwaniNeural\",\n",")\n","\n","language = \"gu-IN\"\n","speech_config.speech_synthesis_language = language\n","# The language of the voice that speaks.\n","speech_config.speech_synthesis_voice_name = \"gu-IN-DhwaniNeural\"\n","\n","\n","def generate_transcription(text: str, output_wave_file: str) -> None:\n","    audio_config = speechsdk.audio.AudioOutputConfig(\n","        filename=output_wave_file,\n","    )\n","\n","    speech_synthesizer = speechsdk.SpeechSynthesizer(\n","        speech_config=speech_config, audio_config=audio_config\n","    )\n","\n","    ssml = (\n","        \"\"\"\n","    <!--ID=B7267351-473F-409D-9765-754A8EBCDE05;Version=1|{\n","        \"VoiceNameToIdMapItems\":[\n","            {\n","                \"Name\": \"Microsoft Server Speech Text to Speech Voice (gu-IN, DhwaniNeural)\",\n","                \"ShortName\": \"gu-IN-DhwaniNeural\",\n","                \"Locale\": \"gu-IN\",\n","                \"Id\": \"97ebc7c6-1e92-4764-806b-ad61201a60a5\",\n","                \"VoiceType\": \"StandardVoice\"\n","            }\n","        ]\n","    }-->\n","    <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\"\n","        xmlns:mstts=\"http://www.w3.org/2001/mstts\" xmlns:emo=\"http://www.w3.org/2009/10/emotionml\"\n","        xml:lang=\"gu-IN\">\n","        <voice name=\"gu-IN-DhwaniNeural\">\n","            \"\"\"\n","        + text\n","        + \"\"\"\n","        </voice>\n","    </speak>\n","    \"\"\"\n","    )\n","\n","    speech_synthesis_result = speech_synthesizer.speak_ssml_async(ssml=ssml).get()\n","\n","    if (\n","        speech_synthesis_result.reason\n","        == speechsdk.ResultReason.SynthesizingAudioCompleted\n","    ):\n","        pass\n","        # print(f\"Audio transcription was successful!\")\n","        # with wave.open(output_wave_file, \"wb\") as out_f:\n","        #     out_f.setnchannels(1)\n","        #     out_f.setsampwidth(2) # number of bytes\n","        #     out_f.setframerate(44100)\n","        #     out_f.writeframesraw(speech_synthesis_result.audio_data)\n","    elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n","        cancellation_details = speech_synthesis_result.cancellation_details\n","        print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n","        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n","            if cancellation_details.error_details:\n","                print(\"Error details: {}\".format(cancellation_details.error_details))\n","                print(\"Did you set the speech resource key and region values?\")\n"]},{"cell_type":"markdown","metadata":{},"source":[" ## Sync `data` from `blob` to `local` folder"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# if SYNC_BLOB_TO_LOCAL_FORCE_DOWNLOAD:\n","#     # Function reference: https://learn.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python#azure-storage-blob-blobserviceclient-get-container-client\n","#     container_client = blob_service_client.get_container_client(\n","#         container=CONTAINER_NAME\n","#     )\n","\n","#     for blob_name in tqdm(\n","#         iterable=container_client.list_blob_names(name_starts_with=\"data\")\n","#     ):\n","#         # Function reference: https://learn.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python#azure-storage-blob-blobserviceclient-get-blob-client\n","#         blob_client = blob_service_client.get_blob_client(\n","#             container=CONTAINER_NAME, blob=blob_name\n","#         )\n","#         blob_name_dirname = os.path.dirname(blob_name)\n","#         os.makedirs(name=blob_name_dirname, exist_ok=True)\n","#         if not os.path.exists(blob_name):\n","#             with open(blob_name, \"wb\") as local_file:\n","#                 download_stream = blob_client.download_blob()\n","#                 local_file.write(download_stream.readall())\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Content Creation Pipeline"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Steps\n","\n","1. Get a Gujarati Kahevat in Gujarati with English Meaning\n","2. Get a Background Image from ...\n","3. Get a Background Music from ..."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1. Gujarati Kahevat"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n","\n","response = openai.Completion.create(\n","  model=\"text-davinci-003\",\n","  prompt=\"Popular Gujarati Kahevat in Gujarati with Meaning in English\",\n","  temperature=0.7,\n","  max_tokens=999,\n","  top_p=1,\n","  frequency_penalty=0,\n","  presence_penalty=0\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"stop\",\n","      \"index\": 0,\n","      \"logprobs\": null,\n","      \"text\": \"\\n\\n1. \\u0aa8\\u0ab0 \\u0aa8\\u0ab0\\u0abe\\u0a9c\\u0aa8\\u0ac1\\u0a82 \\u0aac\\u0ab9\\u0abe\\u0ab0 \\u0a85\\u0aa8\\u0ac7 \\u0aa8\\u0abe\\u0ab0\\u0abe\\u0aaf\\u0aa3\\u0aa8\\u0ac1\\u0a82 \\u0aa8\\u0a9c\\u0ab0 \\u0aa4\\u0ab0\\u0aab \\u0aa8\\u0ab9\\u0ac0:\\n\\nTranslation: No one is greater than the King, and no one should look down upon the poor.\"\n","    }\n","  ],\n","  \"created\": 1674227599,\n","  \"id\": \"cmpl-6anMtx2e9D9nA2tZIn5WWUv1ZiUoP\",\n","  \"model\": \"text-davinci-003\",\n","  \"object\": \"text_completion\",\n","  \"usage\": {\n","    \"completion_tokens\": 137,\n","    \"prompt_tokens\": 15,\n","    \"total_tokens\": 152\n","  }\n","}\n"]}],"source":["print(response)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","1. નર નરાજનું બહાર અને નારાયણનું નજર તરફ નહી:\n","\n","Translation: No one is greater than the King, and no one should look down upon the poor.\n"]}],"source":["print(response['choices'][0]['text'])"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["નર નરાજનું બહાર અને નારાયણનું નજર તરફ નહી: \n"," Translation: No one is greater than the King, and no one should look down upon the poor.\n"]}],"source":["result_split = response['choices'][0]['text'].split('\\n')\n","guj_kvt = result_split[2].split('1.')[1].strip()\n","eng_mean = result_split[4].strip()\n","print(guj_kvt, '\\n', eng_mean)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 2. Image from Bing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["content_creation_pipeline_folder = \"data/\" # add number folder\n","\n","search_url = f\"{os.environ['bing_endpoint']}v7.0/images/search\"\n","subscription_key = os.environ['bing_subscription_key']\n","            \n","headers = {\n","    \"Ocp-Apim-Subscription-Key\" : subscription_key,\n","}\n","params = {\n","    \"q\": \"inspiration, motivation\", # Can also search for specific images\n","    \"license\": \"all\",\n","    \"imageType\": \"photo\",\n","    \"modules\": \"Collections, BRQ, SimilarProducts, SimilarImages, RecognizedEntities, RelatedSearches\", # Bing API insight token: https://learn.microsoft.com/en-us/bing/search-apis/bing-image-search/reference/query-parameters#insightstoken\n","    \"cc\": \"IN\", # Bing API Country Codes https://learn.microsoft.com/en-us/bing/search-apis/bing-image-search/reference/market-codes#country-codes\n","    #\"setLang\": \"gu\", # Bing API language codes https://learn.microsoft.com/en-us/bing/search-apis/bing-image-search/reference/market-codes#bing-supported-language-codes\n","}\n","\n","response = requests.get(search_url, headers=headers, params=params)\n","response.raise_for_status()\n","search_results = response.json()\n","\n","image_num = 0\n","nextOffset = search_results[\"nextOffset\"]\n","os.makedirs(name=content_creation_pipeline_folder, exist_ok=True)\n","\n","while image_num == 0:\n","    search_results[\"value\"].sort(key=lambda element: element['datePublished'], reverse=True)\n","\n","    for i in range(len(search_results)):\n","        if search_results[\"value\"][i][\"isFamilyFriendly\"] == True:\n","            if \"youtube\" not in search_results[\"value\"][i][\"hostPageUrl\"]:\n","                if image_num == 0:\n","                    image_num += 1\n","\n","                    image_data = requests.get(search_results[\"value\"][i][\"thumbnailUrl\"])\n","                    image_data.raise_for_status()\n","                    image = Image.open(BytesIO(image_data.content))\n","                    image.save(f\"{content_creation_pipeline_folder}/image.jpg\")\n","\n","                else:\n","                    break"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 3. Music"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["content_creation_pipeline_folder = \"data/\" # add number folder\n","\n","# Generate audio file describing the word whilst spoken in Gujarati by Azure Speech API (saved as wav file)\n","wave_file_output = f\"{content_creation_pipeline_folder}/gujaratiDefinition.wav\"\n","if not os.path.exists(wave_file_output):\n","    generate_transcription(\n","        text=guj_kvt,\n","        output_wave_file=wave_file_output,\n","    )"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'magenta'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Look at dynamic music synthesis models (e.g. Google Magenta) to produce some relaxing sounds\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmagenta\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmagenta\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmusic_vae\u001b[39;00m \u001b[39mimport\u001b[39;00m configs\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmagenta\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmusic_vae\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrained_model\u001b[39;00m \u001b[39mimport\u001b[39;00m TrainedModel\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'magenta'"]}],"source":["## Look at dynamic music synthesis models (e.g. Google Magenta) to produce some relaxing sounds\n","\n","import magenta\n","from magenta.models.music_vae import configs\n","from magenta.models.music_vae.trained_model import TrainedModel\n","\n","# Load the MusicVAE model\n","config = configs.CONFIG_MAP['cat-mel_2bar_big']\n","model = TrainedModel(config)\n","\n","# Generate a new MIDI file\n","generated_sequence = model.generate(length=16, temperature=1.0)\n","magenta.music.sequence_proto_to_midi_file(generated_sequence, 'generated.mid')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Prepare Content List"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Steps\n","\n","1. Put the Text on Image\n","2. Create a animation of text on Image\n","3. Put Music in background\n","4. Save the video as a mp4"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1. Put Text on Image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from PIL import ImageFont, ImageDraw\n","\n","content_creation_pipeline_folder = \"data/\" # add number folder\n","\n","bg_image = Image.open(f\"{content_creation_pipeline_folder}/image.jpg\")\n","\n","txt = guj_kvt\n","txt_font = ImageFont.truetype('playfair/playfair-font.ttf', 200)    # https://fonts.google.com/\n","txt_pos = (15,15)\n","txt_color = (237, 230, 211)    # https://colorpicker.me/#84f760\n","\n","image_editable = ImageDraw.Draw(bg_image)\n","image_editable.text(xy=txt_pos, text=txt, fill=txt_color, font=txt_font)\n","\n","bg_image.save(f\"{content_creation_pipeline_folder}/result.jpg\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 2. Animation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 3. Background Music Addition"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 4. Video"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from moviepy.editor import *\n","\n","# Load the image\n","image = ImageClip(\"image.jpg\")\n","\n","# Load the background music\n","music = AudioFileClip(\"bg_music.mp3\")\n","\n","# Create the final video by concatenating the image and music\n","final_video = concatenate_videoclips([image, music])\n","\n","# Save the final video\n","final_video.write_videofile(\"video_with_image_and_music.mp4\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Process Content List"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Steps\n","\n","1. Upload to the blob\n","2. Upload to social network\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"eb4a0ac80907d7f44e1a5e88d3d3381b33e3dbedd3a24d113e876f30a0c46bee"}}},"nbformat":4,"nbformat_minor":2}
